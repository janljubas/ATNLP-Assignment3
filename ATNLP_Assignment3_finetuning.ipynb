{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiU9kRaRZc-X"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install git-lfs\n",
        "!git lfs install\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kREMC3_DSgXj",
        "outputId": "43aabedc-ea77-4e10-a1b9-543de092a1ba"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "git-lfs is already the newest version (3.0.2-1ubuntu0.3).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5czm3DzLTW0q",
        "outputId": "9ce91913-24a8-46e6-87f7-07aa638ff308"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ATNLP_Assignment3_finetuning.ipynb  data  LoRA.ipynb  README.md  wandb\n",
            "custom_tokenizer.json\t\t    logs  models      results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf results\n",
        "!rm -rf ATNLP-Assignment3  # Remove any existing folder\n",
        "# !git clone https://github.com/janljubas/ATNLP-Assignment3.git\n"
      ],
      "metadata": {
        "id": "QnaxfS_ptH96"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": true,
        "id": "ztmn7welWy7O",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01f5216b-6aad-4136-c89d-a1401191ccaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.9.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.0.post0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.2)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.5.1+cu121)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2024.9.0)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.6.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.12.2)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.11.9)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.6)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.25.5)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.10.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.19.2)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.1.0)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.11)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2024.12.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.5)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.1.0->pytorch-lightning) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.18.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.27.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2024.12.14)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers datasets\n",
        "!pip install pytorch-lightning wandb\n",
        "!pip install --upgrade transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true,
        "id": "wJ5TaPIVW5nz"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"t5-base\"  # Or \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvq3CKJZraJ"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81h8v8-yr14f"
      },
      "source": [
        "#### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "MrFKuHGXr4cz"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
        "\n",
        "# Create a custom tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())  # Byte-Pair Encoding (BPE) model\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # Pre-tokenize by whitespace\n",
        "tokenizer.decoder = decoders.BPEDecoder()\n",
        "\n",
        "# Train the tokenizer on your data\n",
        "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
        "tokenizer.train(files=[\"/content/ATNLP-Assignment3/data/all_data.txt\"], trainer=trainer)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save(\"/content/ATNLP-Assignment3/exp_2_custom_tokenizer.json\")\n",
        "\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "tokenizer = Tokenizer.from_file(\"/content/ATNLP-Assignment3/exp_2_custom_tokenizer.json\")\n",
        "\n",
        "# Wrap the custom tokenizer\n",
        "custom_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    pad_token=\"<pad>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEyz2TE2r6tt"
      },
      "source": [
        "#### Regular part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TXPbnNpacgR6"
      },
      "outputs": [],
      "source": [
        "# STEP 1: LOADING DATA AND CREATING ALL 3 SPLITS\n",
        "\n",
        "\n",
        "# Load the SCAN dataset\n",
        "def load_scan_data(file_path):\n",
        "    inputs, outputs = [], []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"IN:\"):\n",
        "                # Split the line into input and output parts\n",
        "                parts = line.strip().split(\" OUT: \")\n",
        "                if len(parts) == 2:\n",
        "                    inputs.append(parts[0].replace(\"IN: \", \"\"))  # Remove \"IN: \"\n",
        "                    outputs.append(parts[1])  # Keep the output as is\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.1. loading the training data\n",
        "train_inputs, train_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_train_length.txt\")\n",
        "# print(len(train_inputs), len(train_outputs))\n",
        "\n",
        "# 1.2. creating the Validation set by splitting the train set 90-10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(\n",
        "    train_inputs, train_outputs, test_size=0.02, random_state=42  # so that 0.64 of original train data is now what's left\n",
        ")\n",
        "# print(len(train_inputs), len(train_outputs))\n",
        "# print(len(val_inputs), len(val_outputs))\n",
        "\n",
        "# 1.3. loading the test data\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_test_length.txt\")\n",
        "# print(len(test_inputs), len(test_outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "6EqwwctwgtRe"
      },
      "outputs": [],
      "source": [
        "# STEP 2: PREPROCESSING THE DATA\n",
        "from datasets import Dataset\n",
        "\n",
        "# tokenize and format the data\n",
        "def preprocess_scan_data(inputs, outputs, tokenizer, max_length=48):\n",
        "    formatted_inputs = [f\"translate English to Action: {input_text}\" for input_text in inputs]\n",
        "    model_inputs = tokenizer(formatted_inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=max_length, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# preprocess training, validation, and test data\n",
        "train_data = preprocess_scan_data(train_inputs, train_outputs, custom_tokenizer)\n",
        "val_data = preprocess_scan_data(val_inputs, val_outputs, custom_tokenizer)\n",
        "test_data = preprocess_scan_data(test_inputs, test_outputs, custom_tokenizer)\n",
        "\n",
        "# create Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n",
        "test_dataset = Dataset.from_dict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "collapsed": true,
        "id": "6kfHa0laZ3MH",
        "outputId": "5eb812d2-d799-4f99-f257-606b469518c8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-60-04cdd7a49329>:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='6246' max='6246' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6246/6246 14:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>0.002787</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001900</td>\n",
              "      <td>0.000600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000115</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='490' max='490' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [490/490 00:17]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results: {'eval_loss': 0.08533097058534622, 'eval_runtime': 17.1776, 'eval_samples_per_second': 228.204, 'eval_steps_per_second': 28.526, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "from transformers import TrainerCallback\n",
        "# Custom callback to rename checkpoints\n",
        "class RenameCheckpointCallback(TrainerCallback):\n",
        "    def on_save(self, args, state, control, **kwargs):\n",
        "        import os\n",
        "        checkpoint_dir = os.path.join(args.output_dir, f\"checkpoint-{state.global_step}\")\n",
        "        new_checkpoint_dir = os.path.join(args.output_dir, f\"experiment_1_checkpoint_{state.global_step}\")\n",
        "        os.rename(checkpoint_dir, new_checkpoint_dir)\n",
        "        return control\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-4,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=32,\n",
        "    save_steps=10000,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# init the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,  # Use validation set if available\n",
        "    tokenizer=custom_tokenizer,\n",
        "    callbacks=[RenameCheckpointCallback()]\n",
        ")\n",
        "\n",
        "!rm -rf results\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# model evaluation using the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\", test_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuZKr0rFxaVy",
        "outputId": "83de2ffe-d1a9-496a-85f0-38f4d2eecea1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/ATNLP-Assignment3/exp2_custom_tokenizer/tokenizer_config.json',\n",
              " '/content/ATNLP-Assignment3/exp2_custom_tokenizer/special_tokens_map.json',\n",
              " '/content/ATNLP-Assignment3/exp2_custom_tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"/content/ATNLP-Assignment3/results/experiment_2\")\n",
        "custom_tokenizer.save_pretrained(\"/content/ATNLP-Assignment3/exp2_custom_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "Wor8bAJ9lpE2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First approach"
      ],
      "metadata": {
        "id": "G0ghql3alufN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1UCERU1QBOD",
        "outputId": "a673f11e-360b-4947-a814-b95ea9ff42c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: run around left twice after jump around left\n",
            "Predicted Output: I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN I_TURN_LEFT I_RUN\n"
          ]
        }
      ],
      "source": [
        "# Post-processing\n",
        "def add_spaces_to_output(output_text):\n",
        "    \"\"\"\n",
        "    Adds spaces before each occurrence of \"I_\" (except the first one).\n",
        "    \"\"\"\n",
        "    # Split the output text into parts based on \"I_\"\n",
        "    parts = output_text.split(\"I_\")\n",
        "\n",
        "    # Reconstruct the text with spaces before \"I_\" (except the first part)\n",
        "    reconstructed_text = parts[0]  # First part doesn't need a space\n",
        "    for part in parts[1:]:\n",
        "        reconstructed_text += \" I_\" + part\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "# Test the model on a custom input\n",
        "def model_prediction(input_text):\n",
        "    # Ensure model is on the correct device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    input_ids = custom_tokenizer(f\"translate English to Action: {input_text}\", return_tensors=\"pt\").input_ids.to(device\n",
        "                                                                                                                 )\n",
        "    outputs = model.generate(input_ids)\n",
        "    decoded_output = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    decoded_output_with_spaces = add_spaces_to_output(decoded_output)\n",
        "    return decoded_output_with_spaces\n",
        "\n",
        "# Example test\n",
        "test_input = \"run around left twice after jump around left\"\n",
        "predicted_output = model_prediction(test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Predicted Output: {predicted_output[1:]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HT4OgLDwzTTW",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0949b4f0-020d-4d49-9d22-962c591cd366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 123/123 [24:30<00:00, 11.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on Test Set: 0.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ],
      "source": [
        "# Evaluate the model on the test set\n",
        "def evaluate_on_test_set(test_inputs, test_outputs, batch_size=32):\n",
        "    \"\"\"\n",
        "    Evaluate the model on the test set using batch processing.\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = len(test_outputs)\n",
        "\n",
        "    # Split inputs and outputs into batches\n",
        "    batches = [\n",
        "        (test_inputs[i:i + batch_size], test_outputs[i:i + batch_size])\n",
        "        for i in range(0, total, batch_size)\n",
        "    ]\n",
        "\n",
        "    for input_batch, output_batch in tqdm(batches):\n",
        "        # Use model_prediction for each input in the batch\n",
        "        predicted_outputs = [model_prediction(input_text) for input_text in input_batch]\n",
        "\n",
        "        # Compare predicted outputs to expected outputs\n",
        "        correct += sum(\n",
        "            pred.strip() == exp.strip()\n",
        "            for pred, exp in zip(predicted_outputs, output_batch)\n",
        "        )\n",
        "\n",
        "    accuracy = correct / total\n",
        "    print(f\"Accuracy on Test Set: {accuracy * 100:.2f}%\")\n",
        "    return accuracy\n",
        "\n",
        "# Load test set\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_test_length.txt\")\n",
        "\n",
        "# Evaluate\n",
        "evaluate_on_test_set(test_inputs, test_outputs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_input = \"run thrice and jump left thrice\"\n",
        "predicted_output = model_prediction(test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Predicted Output: {predicted_output}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9ayY-F4lRRR",
        "outputId": "0a371850-7581-433a-fa03-4c4e6b312b1a"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: run thrice and jump left thrice\n",
            "Predicted Output:  I_RUN I_RUN I_RUN I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## token-level and sequence-level accuracy"
      ],
      "metadata": {
        "id": "xxDL2RLZw9j9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Post-processing\n",
        "def add_spaces_to_output(output_text):\n",
        "    \"\"\"\n",
        "    Adds spaces before each occurrence of \"I_\" (except the first one).\n",
        "    \"\"\"\n",
        "    # Split the output text into parts based on \"I_\"\n",
        "    parts = output_text.split(\"I_\")\n",
        "\n",
        "    # Reconstruct the text with spaces before \"I_\" (except the first part)\n",
        "    reconstructed_text = parts[0]  # First part doesn't need a space\n",
        "    for part in parts[1:]:\n",
        "        reconstructed_text += \" I_\" + part\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "# Generate predictions\n",
        "def generate_predictions(input_text):\n",
        "    input_ids = custom_tokenizer(f\"translate English to Action: {input_text}\", return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    decoded_output = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    decoded_output_with_spaces = add_spaces_to_output(decoded_output)\n",
        "    return decoded_output_with_spaces[1:]\n",
        "\n"
      ],
      "metadata": {
        "id": "vB98pXbZxWBv"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "tokenizer = custom_tokenizer\n",
        "\n",
        "\n",
        "def token_lvl_accuracy(gt, pred):\n",
        "    \"\"\"\n",
        "    gt = ground truth sequence (str)\n",
        "    pred = predicted sequence (str)\n",
        "    \"\"\"\n",
        "    # Tokenize the ground truth and predictions\n",
        "    prediction_tokens = custom_tokenizer.encode(prediction, add_special_tokens=True)\n",
        "    gt_tokens = custom_tokenizer.encode(ground_truths, add_special_tokens=True)\n",
        "    print(\"Encoded Ground Truth:\", gt_tokens)\n",
        "    print(\"Encoded Prediction:\", prediction_tokens)\n",
        "\n",
        "    # Find the length of the shorter sequence\n",
        "    min_length = min(len(gt_tokens), len(prediction_tokens))\n",
        "    print(min_length)\n",
        "\n",
        "    # Trim sequences to the same length for comparison\n",
        "    gt_tokens = gt_tokens[:min_length]\n",
        "    pred_tokens = prediction_tokens[:min_length]\n",
        "\n",
        "    # Compare tokens and calculate accuracy\n",
        "    correct = sum(g == p for g, p in zip(gt_tokens, prediction_tokens))\n",
        "    accuracy = correct / len(gt_tokens)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def sequence_level_accuracy(gt, pred):\n",
        "    \"\"\"\n",
        "    gt = ground truth sequence (str)\n",
        "    pred = predicted sequence (str)\n",
        "    \"\"\"\n",
        "    # Check if the ground truth matches the prediction exactly\n",
        "    return int(gt.strip() == pred.strip())\n"
      ],
      "metadata": {
        "id": "3U9im171xB7J"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "k-cIO2PwZzBY"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# !pip install tqdm\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/ATNLP-Assignment3/results/experiment_2\")\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/ATNLP/fine_tuned_t5_base_model/10_epochs_custom_tokenizer\")\n",
        "tokenizer = custom_tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Evaluate accuracy\n",
        "def token_level_accuracy(prediction, ground_truth):\n",
        "    matches = sum(1 for a, b in zip(prediction, ground_truth) if a == b)\n",
        "    percentage = (matches / len(prediction)) * 100\n",
        "    return percentage\n",
        "\n",
        "matches = list()\n",
        "\n",
        "# Load test set\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_test_length.txt\")\n",
        "\n",
        "test_inputs_2 = test_inputs[:200]\n",
        "test_outputs_2 = test_outputs[:200]\n",
        "\n",
        "for i in tqdm(range(len(test_inputs_2)), desc=\"Processing\"):\n",
        "  prediction = generate_predictions(test_inputs_2[i])\n",
        "  ground_truth = test_outputs_2[i]\n",
        "\n",
        "  min_len = min(len(prediction), len(ground_truth))\n",
        "\n",
        "  prediction = prediction[:min_len]\n",
        "  ground_truth = ground_truth[:min_len]\n",
        "\n",
        "  matches.append(token_level_accuracy(custom_tokenizer.encode(prediction, add_special_tokens=True), custom_tokenizer.encode(ground_truth, add_special_tokens=True)))\n",
        "\n",
        "print(f\"\\nToken-level accuracy: {sum(matches) / len(matches)}\")"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cynq_M1W0J6k",
        "outputId": "22ac3586-ebdb-4b2d-8927-7c280dc006be"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:14<00:00,  1.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Token-level accuracy: 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example test\n",
        "test_input = \"look around left twice after jump around right\"\n",
        "predicted_output = model_prediction(test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Predicted Output: {predicted_output[1:]}\")"
      ],
      "metadata": {
        "id": "JhOF4FDXk1Dc",
        "outputId": "31a437d3-0791-49bc-b668-c6de7934d41a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: look around left twice after jump around right\n",
            "Predicted Output: I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_RIGHT I_JUMP I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK I_TURN_LEFT I_LOOK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQNHx-HZqcZ"
      },
      "source": [
        "\n",
        "# Experiment 1 stuff\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# EXPERIMENT 1\n",
        "\n",
        "def token_lvl_accuracy(gt, pred):\n",
        "    \"\"\"\n",
        "    gt = ground truth sequence\n",
        "    pred = predicted sequence\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "\n",
        "    # get start and end\n",
        "    eos_idx = word2idx_tgt['<EOS>']\n",
        "    sos_idx = word2idx_tgt['<SOS>']\n",
        "    # print(eos_idx)\n",
        "    # print(sos_idx)\n",
        "    pred = pred[-1]\n",
        "\n",
        "\n",
        "    gt = gt[-1]\n",
        "\n",
        "    # index of <SOS> and <EOS> tokens of the predicted sequence\n",
        "    pred_start = 0\n",
        "    pred_end = len(pred) if (eos_idx not in pred) else (pred == eos_idx).nonzero(as_tuple=True)[0].item()\n",
        "\n",
        "    # index of <SOS> and <EOS> tokens of the ground truth sequence\n",
        "    gt_start = (gt == sos_idx).nonzero(as_tuple=True)[0].item()\n",
        "    gt_end = (gt == eos_idx).nonzero(as_tuple=True)[0].item()\n",
        "\n",
        "    # slicing\n",
        "    gt = gt[gt_start+1 : gt_end]\n",
        "    pred = pred[pred_start+1 : pred_end]\n",
        "\n",
        "    longer = gt if len(gt) > len(pred) else pred\n",
        "    shorter = pred if len(gt) > len(pred) else gt\n",
        "\n",
        "    longest_len = len(longer)\n",
        "\n",
        "    shorter = torch.nn.functional.pad(shorter, (0, longest_len - len(shorter)), \"constant\", 0)\n",
        "\n",
        "    correct = sum(longer == shorter)\n",
        "    # print(longer)\n",
        "    # print(shorter)\n",
        "    # print(correct)\n",
        "    return int(correct) / len(shorter) # same length as longer\n",
        "\n",
        "\n",
        "def sequence_level_accuracy(gt, pred):\n",
        "\n",
        "    pred = pred[-1]\n",
        "    gt = gt[-1]\n",
        "\n",
        "    if len(gt) != len(pred):\n",
        "        return 0\n",
        "\n",
        "    if sum(gt == pred) == len(gt):\n",
        "        return 1\n",
        "\n",
        "    return 0"
      ],
      "metadata": {
        "id": "eqnIpoxtvnyH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2 stuff"
      ],
      "metadata": {
        "id": "dBa80Nmbl4ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define token-level accuracy function\n",
        "def token_level_accuracy(prediction_tokens, ground_truth_tokens):\n",
        "    \"\"\"\n",
        "    Compute the percentage of matching tokens between the prediction and ground truth.\n",
        "\n",
        "    Args:\n",
        "    prediction_tokens: List[int] - Tokenized prediction sequence.\n",
        "    ground_truth_tokens: List[int] - Tokenized ground truth sequence.\n",
        "\n",
        "    Returns:\n",
        "    float - Accuracy as a percentage.\n",
        "    \"\"\"\n",
        "    # Ensure both sequences are the same length for comparison\n",
        "    min_length = min(len(prediction_tokens), len(ground_truth_tokens))\n",
        "    max_length = max(len(prediction_tokens), len(ground_truth_tokens))\n",
        "    prediction_tokens = prediction_tokens[:min_length]\n",
        "    ground_truth_tokens = ground_truth_tokens[:min_length]\n",
        "\n",
        "    # Calculate the number of matches\n",
        "    matches = sum(1 for pred, gt in zip(prediction_tokens, ground_truth_tokens) if pred == gt)\n",
        "    accuracy = (matches / min_length) * 100\n",
        "    return accuracy\n",
        "\n",
        "# Evaluate on test set\n",
        "matches = []\n",
        "\n",
        "# Load the test dataset\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_test_length.txt\")\n",
        "\n",
        "# Use a smaller subset for testing (adjust as needed)\n",
        "test_inputs_subset = test_inputs[:100]\n",
        "test_outputs_subset = test_outputs[:100]\n",
        "\n",
        "for i in tqdm(range(len(test_inputs_subset)), desc=\"Processing\"):\n",
        "    # Generate predictions for each input\n",
        "    prediction = model_prediction(test_inputs_subset[i])\n",
        "    ground_truth = test_outputs_subset[i]\n",
        "\n",
        "    # Tokenize predictions and ground truth\n",
        "    prediction_tokens = custom_tokenizer.encode(prediction, add_special_tokens=True)\n",
        "    ground_truth_tokens = custom_tokenizer.encode(ground_truth, add_special_tokens=True)\n",
        "\n",
        "    # Calculate token-level accuracy\n",
        "    accuracy = token_level_accuracy(prediction_tokens, ground_truth_tokens)\n",
        "    matches.append(accuracy)\n",
        "\n",
        "# Compute the average token-level accuracy\n",
        "average_accuracy = sum(matches) / len(matches)\n",
        "print(f\"\\nAverage Token-Level Accuracy: {average_accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "uoGqh8Tjl56S",
        "outputId": "0a0bbdcd-ee2c-4e2d-fc05-75808b9d3ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:38<00:00,  2.59it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Token-Level Accuracy: 100.00%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Define sequence-level accuracy function without trimming\n",
        "def sequence_level_accuracy_no_trimming(prediction, ground_truth):\n",
        "    \"\"\"\n",
        "    Compute sequence-level accuracy without trimming.\n",
        "    If the prediction is shorter or longer than the ground truth, it is considered incorrect.\n",
        "\n",
        "    Args:\n",
        "    prediction: str - Predicted sequence.\n",
        "    ground_truth: str - Ground truth sequence.\n",
        "\n",
        "    Returns:\n",
        "    int - 1 if sequences match exactly (including length), 0 otherwise.\n",
        "    \"\"\"\n",
        "    return int(prediction.strip() == ground_truth.strip())\n",
        "\n",
        "# Evaluate on test set\n",
        "matches = []\n",
        "\n",
        "# Load the test dataset\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-2/tasks_test_length.txt\")\n",
        "\n",
        "# Use a smaller subset for testing (adjust as needed)\n",
        "# test_inputs_subset = test_inputs[:200]\n",
        "# test_outputs_subset = test_outputs[:200]\n",
        "\n",
        "for i in tqdm(range(len(test_inputs_subset)), desc=\"Processing\"):\n",
        "    # Generate predictions for each input\n",
        "    prediction = model_prediction(test_inputs_subset[i])\n",
        "    ground_truth = test_outputs_subset[i]\n",
        "\n",
        "    # Calculate sequence-level accuracy without trimming\n",
        "    accuracy = sequence_level_accuracy_no_trimming(prediction, ground_truth)\n",
        "    matches.append(accuracy)\n",
        "\n",
        "# Compute the average sequence-level accuracy\n",
        "average_accuracy = sum(matches) / len(matches)\n",
        "print(f\"\\nAverage Sequence-Level Accuracy (No Trimming): {average_accuracy:.2f}\")\n"
      ],
      "metadata": {
        "id": "JrVceDnUm8IU",
        "outputId": "6a3698bb-58d1-4ced-8ea6-e27246b87266",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 200/200 [01:16<00:00,  2.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Average Sequence-Level Accuracy (No Trimming): 0.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZEyz2TE2r6tt"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "17pwvX2mYdUgujtOgfjAifs_bPqubQ_Fo",
      "authorship_tag": "ABX9TyNeogzHnnvbDbZgT05JIi69"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}