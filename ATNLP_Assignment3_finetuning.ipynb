{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jiU9kRaRZc-X"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cd ATNLP-Assignment3/"
      ],
      "metadata": {
        "id": "QnaxfS_ptH96"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ztmn7welWy7O"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install transformers datasets\n",
        "!pip install pytorch-lightning wandb\n",
        "!pip install --upgrade transformers torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wJ5TaPIVW5nz"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "from transformers import T5ForConditionalGeneration, TrainingArguments, Trainer\n",
        "\n",
        "model_name = \"t5-base\"  # Or \"t5-small\"\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVvq3CKJZraJ"
      },
      "source": [
        "# Dataset preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81h8v8-yr14f"
      },
      "source": [
        "#### Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "MrFKuHGXr4cz"
      },
      "outputs": [],
      "source": [
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers\n",
        "\n",
        "# Create a custom tokenizer\n",
        "tokenizer = Tokenizer(models.BPE())  # Byte-Pair Encoding (BPE) model\n",
        "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()  # Pre-tokenize by whitespace\n",
        "tokenizer.decoder = decoders.BPEDecoder()\n",
        "\n",
        "# Train the tokenizer on your data\n",
        "trainer = trainers.BpeTrainer(vocab_size=5000, special_tokens=[\"<pad>\", \"<unk>\", \"<s>\", \"</s>\"])\n",
        "tokenizer.train(files=[\"/content/ATNLP-Assignment3/data/all_data.txt\"], trainer=trainer)\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save(\"/content/ATNLP-Assignment3/custom_tokenizer.json\")\n",
        "\n",
        "\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "# tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "tokenizer = Tokenizer.from_file(\"/content/ATNLP-Assignment3/custom_tokenizer.json\")\n",
        "\n",
        "# Wrap the custom tokenizer\n",
        "custom_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    pad_token=\"<pad>\",\n",
        "    unk_token=\"<unk>\",\n",
        "    bos_token=\"<s>\",\n",
        "    eos_token=\"</s>\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEyz2TE2r6tt"
      },
      "source": [
        "#### Regular part"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TXPbnNpacgR6"
      },
      "outputs": [],
      "source": [
        "# STEP 1: LOADING DATA AND CREATING ALL 3 SPLITS\n",
        "\n",
        "\n",
        "# Load the SCAN dataset\n",
        "def load_scan_data(file_path):\n",
        "    inputs, outputs = [], []\n",
        "    with open(file_path, 'r') as f:\n",
        "        for line in f:\n",
        "            if line.startswith(\"IN:\"):\n",
        "                # Split the line into input and output parts\n",
        "                parts = line.strip().split(\" OUT: \")\n",
        "                if len(parts) == 2:\n",
        "                    inputs.append(parts[0].replace(\"IN: \", \"\"))  # Remove \"IN: \"\n",
        "                    outputs.append(parts[1])  # Keep the output as is\n",
        "    return inputs, outputs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 1.1. loading the training data\n",
        "train_inputs, train_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-1/tasks_train_simple.txt\")\n",
        "# print(len(train_inputs), len(train_outputs))\n",
        "\n",
        "# 1.2. creating the Validation set by splitting the train set 90-10\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_inputs, val_inputs, train_outputs, val_outputs = train_test_split(\n",
        "    train_inputs, train_outputs, test_size=0.1, random_state=42\n",
        ")\n",
        "# print(len(train_inputs), len(train_outputs))\n",
        "# print(len(val_inputs), len(val_outputs))\n",
        "\n",
        "# 1.3. loading the test data\n",
        "test_inputs, test_outputs = load_scan_data(\"/content/ATNLP-Assignment3/data/Experiment-1/tasks_test_simple.txt\")\n",
        "# print(len(test_inputs), len(test_outputs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6EqwwctwgtRe"
      },
      "outputs": [],
      "source": [
        "# STEP 2: PREPROCESSING THE DATA\n",
        "from datasets import Dataset\n",
        "\n",
        "# tokenize and format the data\n",
        "def preprocess_scan_data(inputs, outputs, tokenizer, max_length=128):\n",
        "    formatted_inputs = [f\"translate English to Action: {input_text}\" for input_text in inputs]\n",
        "    model_inputs = tokenizer(formatted_inputs, max_length=max_length, truncation=True, padding=\"max_length\")\n",
        "    labels = tokenizer(outputs, max_length=max_length, truncation=True, padding=\"max_length\").input_ids\n",
        "    model_inputs[\"labels\"] = labels\n",
        "    return model_inputs\n",
        "\n",
        "# preprocess training, validation, and test data\n",
        "train_data = preprocess_scan_data(train_inputs, train_outputs, custom_tokenizer)\n",
        "val_data = preprocess_scan_data(val_inputs, val_outputs, custom_tokenizer)\n",
        "test_data = preprocess_scan_data(test_inputs, test_outputs, custom_tokenizer)\n",
        "\n",
        "# create Hugging Face Dataset objects\n",
        "train_dataset = Dataset.from_dict(train_data)\n",
        "val_dataset = Dataset.from_dict(val_data)\n",
        "test_dataset = Dataset.from_dict(test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "collapsed": true,
        "id": "6kfHa0laZ3MH",
        "outputId": "79ca8ced-2bcf-4e43-9afd-094cb4575c28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-85-69465114be6f>:16: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5646' max='5646' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5646/5646 15:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.043600</td>\n",
              "      <td>0.029667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.013308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.012200</td>\n",
              "      <td>0.018626</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='523' max='523' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [523/523 00:20]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Results: {'eval_loss': 0.018611395731568336, 'eval_runtime': 20.9624, 'eval_samples_per_second': 199.5, 'eval_steps_per_second': 24.949, 'epoch': 3.0}\n"
          ]
        }
      ],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    save_steps=500,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        ")\n",
        "\n",
        "# init the trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=val_dataset,  # Use validation set if available\n",
        "    tokenizer=custom_tokenizer,\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# model evaluation using the test set\n",
        "test_results = trainer.evaluate(test_dataset)\n",
        "print(\"Test Results:\", test_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuZKr0rFxaVy",
        "outputId": "ef818356-19b0-4617-e487-76700305d740"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/ATNLP/fine_tuned_t5_custom_model/10_epochs_custom_tokenizer/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/ATNLP/fine_tuned_t5_custom_model/10_epochs_custom_tokenizer/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/ATNLP/fine_tuned_t5_custom_model/10_epochs_custom_tokenizer/tokenizer.json')"
            ]
          },
          "execution_count": 93,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and tokenizer\n",
        "model.save_pretrained(\"/content/drive/MyDrive/ATNLP/fine_tuned_t5_custom_model/10_epochs_custom_tokenizer\")\n",
        "custom_tokenizer.save_pretrained(\"/content/drive/MyDrive/ATNLP/fine_tuned_t5_custom_model/10_epochs_custom_tokenizer\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOQNHx-HZqcZ"
      },
      "source": [
        "\n",
        "# Experiments\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "k-cIO2PwZzBY"
      },
      "outputs": [],
      "source": [
        "# Load the fine-tuned model and tokenizer\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"/content/ATNLP-Assignment3/models/t5_base/10_epochs_custom_tokenizer\")\n",
        "# tokenizer = T5Tokenizer.from_pretrained(\"/content/drive/MyDrive/ATNLP/fine_tuned_t5_base_model/10_epochs_custom_tokenizer\")\n",
        "tokenizer = custom_tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1UCERU1QBOD",
        "outputId": "d6e4bade-0987-40d3-f59a-8c6135505af7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: run thrice and jump left thrice\n",
            "Predicted Output:  I_RUN I_RUN I_RUN I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP I_TURN_LEFT I_JUMP\n"
          ]
        }
      ],
      "source": [
        "# Post-processing\n",
        "def add_spaces_to_output(output_text):\n",
        "    \"\"\"\n",
        "    Adds spaces before each occurrence of \"I_\" (except the first one).\n",
        "    \"\"\"\n",
        "    # Split the output text into parts based on \"I_\"\n",
        "    parts = output_text.split(\"I_\")\n",
        "\n",
        "    # Reconstruct the text with spaces before \"I_\" (except the first part)\n",
        "    reconstructed_text = parts[0]  # First part doesn't need a space\n",
        "    for part in parts[1:]:\n",
        "        reconstructed_text += \" I_\" + part\n",
        "\n",
        "    return reconstructed_text\n",
        "\n",
        "# Test the model on a custom input\n",
        "def model_prediction(input_text):\n",
        "    input_ids = custom_tokenizer(f\"translate English to Action: {input_text}\", return_tensors=\"pt\").input_ids\n",
        "    outputs = model.generate(input_ids)\n",
        "    decoded_output = custom_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    decoded_output_with_spaces = add_spaces_to_output(decoded_output)\n",
        "    return decoded_output_with_spaces\n",
        "\n",
        "# Example test\n",
        "test_input = \"run thrice and jump left thrice\"\n",
        "predicted_output = model_prediction(test_input)\n",
        "print(f\"Input: {test_input}\")\n",
        "print(f\"Predicted Output: {predicted_output}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HT4OgLDwzTTW"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "ZEyz2TE2r6tt"
      ],
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "17pwvX2mYdUgujtOgfjAifs_bPqubQ_Fo",
      "authorship_tag": "ABX9TyMwvgX9izBXlSlAXUwis524"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}